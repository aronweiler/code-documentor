<!-- AUTO-GENERATED DOCUMENTATION -->
<!-- This file was automatically generated and should not be manually edited -->
<!-- To update this documentation, regenerate it using the documentation pipeline -->

# Documentation for src\utilities\token_manager.py

# token_manager.py

## Purpose

This module provides a token management system designed for applications that interact with Large Language Models (LLMs) such as GPT-3.5, GPT-4, and others. It enables accurate token counting, budget tracking, and rough context optimization, making it easier to manage LLM context window constraints and cost/budget calculations.

## Functionality

The `TokenCounter` class is the core utility in this file. Its primary responsibilities are:

- **Token Counting**: Accurately count tokens in a text input for specific LLM models using the `tiktoken` library.
- **Model-Specific Encoding**: Use different `tiktoken` encoders for different LLM models, with fallbacks for unknown or unsupported models.
- **Estimation for Chat Messages**: Provide rough token usage estimation for a sequence of conversational messages, including fixed overhead for message structs.

Logging is used internally for error tracking and fallback cases.

## Key Components

### Class: `TokenCounter`

#### Attributes

- `encoders: Dict[str, tiktoken.Encoding]`  
  A dictionary mapping model names to their corresponding `tiktoken` encoder.

#### Methods

- `__init__()`  
  Initializes the class instance and loads all necessary encoders via the `_load_encoders` method.

- `_load_encoders()`  
  Loads `tiktoken` encoders for various well-known models. Stores them in the `encoders` dictionary. Adds a fallback 'default' encoder. Warnings are logged if loading fails.

- `count_tokens(text: str, model: str = "default") -> int`  
  Counts the number of tokens in the provided `text` according to the encoding rules for the specified `model`.  
  - Falls back to a rough estimation if encoding fails or text is empty.
  - Uses the correct encoder by mapping the model name with `_get_encoder_key()`.

- `_get_encoder_key(model: str) -> str`  
  Maps a given model name string to the appropriate encoder key in the `encoders` dictionary.

- `estimate_tokens_for_messages(messages: List[Dict[str, str]], model: str = "default") -> int`  
  Estimates the total token count for a list of messages (e.g., chat history), including a fixed overhead per message and for the overall conversation structure.

### Logging

- The module uses the Python `logging` library for warnings and error tracking during encoder loading and token counting.

## Dependencies

### External

- **[tiktoken](https://pypi.org/project/tiktoken/)**:  
  Provides reliable tokenization strategies for various OpenAI models. Required for accurate token counting.

- **logging** (standard library):  
  Used for status/error messaging.

- **typing** (standard library):  
  For type hints (`Dict`, `List`).

### Internal

- No internal project module dependencies.

### What depends on this module

- Any codebase components requiring accurate or estimated token counts for LLM input, budget control, or prompt/context management.

## Usage Examples

```python
from src.utilities.token_manager import TokenCounter

counter = TokenCounter()

# Count tokens for a string using the GPT-4 encoder
text = "Hello, how can I help you today?"
num_tokens = counter.count_tokens(text, model="gpt-4")
print(f"Tokens in text: {num_tokens}")

# Estimate tokens for a list of chat messages
messages = [
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi! How can I assist you?"},
]
estimated = counter.estimate_tokens_for_messages(messages, model="gpt-3.5-turbo")
print(f"Estimated tokens in conversation: {estimated}")
```

## Notes

- The token counting falls back to an estimate based on word count if the correct encoder is unavailable or an error occurs.
- Fixed overheads are applied when estimating tokens for message arrays, based on OpenAI's chat message structure.
- Only model names like `"gpt-4"`, `"gpt-3.5-turbo"`, `"text-davinci-003"` are mapped directlyâ€”others use `"default"`.

---

*This module aids in making LLM-powered systems more robust and predictable with respect to token usage and budgeting. For usage with a new model, update `_load_encoders` and `_get_encoder_key` as needed.*

---
<!-- GENERATION METADATA -->
```yaml
# Documentation Generation Metadata
file_hash: f712c7ac9c0ca70abd57a06d5ee5bd2587fbbc1ff65cb1c95327d5eb4854f9d9
relative_path: src\utilities\token_manager.py
generation_date: 2025-07-01T22:21:47.691716
```
<!-- END GENERATION METADATA -->
